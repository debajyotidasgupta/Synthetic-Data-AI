{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44a8aa06-5622-4412-90eb-4a37169527cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "\n",
    "from prompt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "268d1617-4202-410b-8926-b11b795bb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "{IMAGE_PLACEHOLDER} \n",
    "\n",
    "Classify this image in one of the following classes\n",
    "- Rufflesia-Arnoldii\n",
    "- Encephalartos-Woodii\n",
    "- Amorphophallus-Titanum\n",
    "- Ghost-Orchid\n",
    "- Dracaena-Cinnabari\n",
    "\n",
    "Only give the class name for the class which you are \n",
    "highly confident the image belongs to.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template = {\n",
    "    \"id\": \"unique_id\",\n",
    "    \"image\": \"image_file.jpg\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": query\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"class\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "121a706b-ec98-4fd0-85c1-9ee13960d295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rufflesia-Arnoldii',\n",
       " 'Encephalartos-Woodii',\n",
       " 'Amorphophallus-Titanum',\n",
       " 'Ghost-Orchid',\n",
       " 'Dracaena-Cinnabari']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list = list(prompt)\n",
    "\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3356320-4637-47f7-b2cb-52809a6ecb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2d49994a744656a5da7c98062b3e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6992bc045a4657bfae84c54d142efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e79b94603f47fca807b9ee79572d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb278c874c1e4e038a73841bc5648d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c634b6cc9ea544cf984d56726227ff1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = []\n",
    "valid_data = []\n",
    "\n",
    "for plant in class_list:\n",
    "    for dir, _, files in os.walk(f'/home/Synthetic-Data-AI/data/synthetic/{plant}'):\n",
    "        for i, file in tqdm(list(enumerate(sorted(files)))):\n",
    "            if file.endswith('.png'):\n",
    "                image_file = os.path.join(plant, file)\n",
    "                input = copy.deepcopy(template)\n",
    "                input['id'] = str(uuid.uuid4())\n",
    "                input['image'] = image_file\n",
    "                input['conversations'][1]['value'] = plant\n",
    "                if i < int(len(files) * 0.8):\n",
    "                    train_data.append(input)\n",
    "                else:\n",
    "                    valid_data.append(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ee27c83-50a3-478b-bc9a-21e6a3fbe2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'db01fe6e-b30f-4db4-804d-4a2f16fdb7ce',\n",
       " 'image': 'Rufflesia-Arnoldii/001.png',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '\\n<image-placeholder> \\n\\nClassify this image in one of the following classes\\n- Rufflesia-Arnoldii\\n- Encephalartos-Woodii\\n- Amorphophallus-Titanum\\n- Ghost-Orchid\\n- Dracaena-Cinnabari\\n\\nOnly give the class name for the class which you are \\nhighly confident the image belongs to.\\n'},\n",
       "  {'from': 'gpt', 'value': 'Rufflesia-Arnoldii'}]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16fad056-46a1-402f-8e66-274c41cddd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/Synthetic-Data-AI/data/train_dataset.json', 'w') as f:\n",
    "    f.write(json.dumps(train_data, indent=4))\n",
    "with open('/home/Synthetic-Data-AI/data/valid_dataset.json', 'w') as f:\n",
    "    f.write(json.dumps(valid_data, indent=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ff1de0e-12ca-4aa8-9afd-11641dfdda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/LLaVA\n"
     ]
    }
   ],
   "source": [
    "%cd ../LLaVA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ded36c42-67f1-4d38-9e3d-270fdf1edd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/finetune_qlora_plant.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/finetune_qlora_plant.sh\n",
    "#!/bin/bash\n",
    "\n",
    "################## VICUNA ##################\n",
    "PROMPT_VERSION=v1\n",
    "MODEL_VERSION=\"vicuna-v1-6-7b\"\n",
    "################## VICUNA ##################\n",
    "\n",
    "deepspeed llava/train/train_mem.py \\\n",
    "    --deepspeed ./scripts/zero2.json \\\n",
    "    --lora_enable True \\\n",
    "    --bits 4 \\\n",
    "    --model_name_or_path \"liuhaotian/llava-v1.6-vicuna-7b\" \\\n",
    "    --version $PROMPT_VERSION \\\n",
    "    --data_path '/home/Synthetic-Data-AI/data/train_dataset.json' \\\n",
    "    --valid_path '/home/Synthetic-Data-AI/data/valid_dataset.json' \\\n",
    "    --image_folder /home/Synthetic-Data-AI/data/synthetic/ \\\n",
    "    --vision_tower \"openai/clip-vit-large-patch14-336\" \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --bf16 True \\\n",
    "    --output_dir ./checkpoints/llava-$MODEL_VERSION-finetune_qlora_plant \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 200 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --report_to wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ce9ee-f34c-4dcf-900e-7ad4ffaf3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./scripts/finetune_qlora_plant.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b9f35765-3309-49b9-b2a7-cb5a7e1feec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d884abf6da34b53972182aeea10f51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e1f0ced37c433d884769eebbf1e462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b6fc93faa44e91b272c50f89dea991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17580a46338643c2b42fe34faf953d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eece157edde4983bb291356a0021985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_shot_data = []\n",
    "valid_shot_data = []\n",
    "\n",
    "for plant in class_list:\n",
    "    for dir, _, files in os.walk(f'/home/Synthetic-Data-AI/data/synthetic-few-shot/5-shot/{plant}'):\n",
    "        for i, file in tqdm(list(enumerate(sorted(files)))):\n",
    "            if file.endswith('.png'):\n",
    "                image_file = os.path.join(plant, file)\n",
    "                input = copy.deepcopy(template)\n",
    "                input['id'] = str(uuid.uuid4())\n",
    "                input['image'] = image_file\n",
    "                input['conversations'][1]['value'] = plant\n",
    "                if i < int(len(files) * 0.8):\n",
    "                    train_shot_data.append(input)\n",
    "                else:\n",
    "                    valid_shot_data.append(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7252b5b-3879-4706-83e9-d10cb12eb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/Synthetic-Data-AI/data/train_5_shot_dataset.json', 'w') as f:\n",
    "    f.write(json.dumps(train_shot_data, indent=4))\n",
    "with open('/home/Synthetic-Data-AI/data/valid_5_shot_dataset.json', 'w') as f:\n",
    "    f.write(json.dumps(valid_shot_data, indent=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b8604a2-95ad-40ed-9f96-9f80adea12fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/finetune_qlora_plant_5_shot.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/finetune_qlora_plant_5_shot.sh\n",
    "#!/bin/bash\n",
    "\n",
    "################## VICUNA ##################\n",
    "PROMPT_VERSION=v1\n",
    "MODEL_VERSION=\"vicuna-v1-6-7b\"\n",
    "################## VICUNA ##################\n",
    "\n",
    "deepspeed llava/train/train_mem.py \\\n",
    "    --deepspeed ./scripts/zero2.json \\\n",
    "    --lora_enable True \\\n",
    "    --lora_r 128 \\\n",
    "    --lora_alpha 256 \\\n",
    "    --bits 4 \\\n",
    "    --model_name_or_path \"liuhaotian/llava-v1.6-vicuna-7b\" \\\n",
    "    --version $PROMPT_VERSION \\\n",
    "    --data_path '/home/Synthetic-Data-AI/data/train_5_shot_dataset.json' \\\n",
    "    --valid_path '/home/Synthetic-Data-AI/data/valid_5_shot_dataset.json' \\\n",
    "    --image_folder /home/Synthetic-Data-AI/data/synthetic/ \\\n",
    "    --vision_tower \"openai/clip-vit-large-patch14-336\" \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --bf16 True \\\n",
    "    --output_dir ./checkpoints/llava-$MODEL_VERSION-5-shot-finetune_qlora_plant_10_epoch \\\n",
    "    --num_train_epochs 20 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 200 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --report_to wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f081138-5488-47a3-9422-517a507721ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
